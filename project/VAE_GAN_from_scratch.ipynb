{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24766,"status":"ok","timestamp":1714319314338,"user":{"displayName":"Sophie Liu","userId":"11072000168371721717"},"user_tz":240},"id":"MFONBW2Xp_db","outputId":"2966c942-dc78-4d76-8f6b-070081f79b32"},"outputs":[],"source":["pip install torch_geometric Bio"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29095,"status":"ok","timestamp":1714319640073,"user":{"displayName":"Sophie Liu","userId":"11072000168371721717"},"user_tz":240},"id":"TzpWNvX2sa-k","outputId":"a2912dc4-1dcf-494f-89e5-3d18fc3470b5"},"outputs":[],"source":["import re\n","import os\n","import numpy as np\n","import pandas as pd\n","import torch\n","import matplotlib.pyplot as plt\n","\n","import torch\n","from torch_geometric.data import Data\n","from torch_geometric.nn import GCNConv\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","from Bio import SeqIO\n","from io import StringIO\n","from itertools import product\n","from tqdm import trange, tqdm\n","from Bio.Seq import Seq\n","from Bio.SeqRecord import SeqRecord\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Define sequence and identifier\n","sequence = \"ATCGATCG\"\n","identifier = \"Sequence1\"\n","\n","# Create a Seq object from the sequence\n","seq_object = Seq(sequence)\n","\n","# Create a SeqRecord object\n","seq_record = SeqRecord(seq_object, id=identifier)\n","\n","print(seq_record)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1714319640074,"user":{"displayName":"Sophie Liu","userId":"11072000168371721717"},"user_tz":240},"id":"fJ9WYmpV07Rd"},"outputs":[],"source":["def get_kmer_dictionary(kmer_size):\n","    alphabet = ['A', 'C', 'G', 'T', 'N']\n","    kmers = [''.join(mer) for mer in product(alphabet, repeat=kmer_size)]\n","    kmer_dict = {kmer: i for i, kmer in enumerate(kmers)}\n","    return kmer_dict\n","\n","def get_edge_index_for_sequence_record(seq_record, kmer_size):\n","  local_dict = get_kmer_dictionary(kmer_size)\n","  # print(local_dict)\n","  sources = []\n","  targets = []\n","  for i in range(0, (len(str(seq_record.seq)) - kmer_size - 1)):\n","    current_block = seq_record.seq[i:i+kmer_size]\n","    target_block = seq_record.seq[i+1:i+kmer_size+1]\n","    current_node = local_dict[current_block]\n","    target_block = local_dict[target_block]\n","\n","    sources.append(current_node)\n","    targets.append(target_block)\n","\n","  return torch.tensor([sources, targets])\n","\n","  def get_edge_index_for_sequence(seq, kmer_size):\n","    print(\"here\")\n","    local_dict = get_kmer_dictionary(kmer_size)\n","    # print(local_dict)\n","    sources = []\n","    targets = []\n","    for i in range(0, (len(seq) - kmer_size - 1)):\n","      current_block = seq[i:i+kmer_size]\n","      target_block = seq[i+1:i+kmer_size+1]\n","      current_node = local_dict[current_block]\n","      target_block = local_dict[target_block]\n","\n","      sources.append(current_node)\n","      targets.append(target_block)\n","\n","    return torch.tensor([sources, targets])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1714319642903,"user":{"displayName":"Sophie Liu","userId":"11072000168371721717"},"user_tz":240},"id":"kj3krUU3E0ah","outputId":"50e61a5e-1ed7-40ef-8700-816ff883096f"},"outputs":[],"source":["%cd /content/drive/MyDrive/10708/10708 Project"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":55690,"status":"ok","timestamp":1714270561825,"user":{"displayName":"Sophie Liu","userId":"11072000168371721717"},"user_tz":240},"id":"pLqiSYHlcczx","outputId":"4c9925d7-fe29-48e0-f277-d94de8d02ebc"},"outputs":[],"source":["#FRAGMENTED SEQUENCES NOT FULL\n","\n","from tqdm import tqdm\n","fasta_file = \"sequences.fasta\"\n","\n","sequences = []\n","records = list(SeqIO.parse(\"sequences.fasta\", \"fasta\"))\n","\n","\n","def get_edge_index_for_sequence(seq, kmer_size):\n","    local_dict = get_kmer_dictionary(kmer_size)\n","    # print(local_dict)\n","    sources = []\n","    targets = []\n","    for i in range(0, (len(seq) - kmer_size - 1)):\n","      current_block = seq[i:i+kmer_size]\n","      target_block = seq[i+1:i+kmer_size+1]\n","      current_node = local_dict[current_block]\n","      target_block = local_dict[target_block]\n","\n","      sources.append(current_node)\n","      targets.append(target_block)\n","\n","    return torch.tensor([sources, targets])\n","def get_subsequences(sequence, subseq_length):\n","    \"\"\"Generate subsequences of the given length from a sequence.\"\"\"\n","    results = []\n","    for i in range(len(sequence) - subseq_length + 1):\n","        if i + subseq_length > len(sequence):\n","          break\n","        results.append(sequence[i:i + subseq_length])\n","    return results\n","\n","train_data = []\n","subseq_length = 100 # Length of each subsequence\n","kmer_size = 3      # Size of k-mer\n","\n","for record in records:\n","    # Retrieve the sequence from the record\n","    sequence = str(record.seq)\n","    # For each subsequence of length 50:\n","    subseqs = get_subsequences(sequence, subseq_length)\n","    for i in tqdm(range(len(subseqs))):\n","        subseq = subseqs[i]\n","        edge_index = get_edge_index_for_sequence(subseq, kmer_size)\n","        train_data.append(edge_index)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"elapsed":1419,"status":"error","timestamp":1714319648603,"user":{"displayName":"Sophie Liu","userId":"11072000168371721717"},"user_tz":240},"id":"Jftp277sEwTe","outputId":"fb504e45-815d-4ab2-a01b-355157c1c164"},"outputs":[],"source":["fasta_file = \"sequences.fasta\"\n","\n","sequences = []\n","records = list(SeqIO.parse(\"sequences.fasta\", \"fasta\"))\n","\n","print(get_edge_index_for_sequence(records[0], 1).shape)\n","\n","train_data = []\n","for record in records:\n","  train_data.append(get_edge_index_for_sequence(record, 2))\n","\n","trunc_size = min(dat.shape[1] for dat in train_data)\n","# print(trunc_size)\n","\n","#Truncate all the sequences to trunc_size - 5\n","for i in range(len(train_data)):\n","  train_data[i] = train_data[i][:,:trunc_size-5]\n","\n","for dat in train_data:\n","  print(dat.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pDbx4U6gsDc0"},"outputs":[],"source":["data_tensor = torch.stack(train_data, dim=0)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1714270610766,"user":{"displayName":"Sophie Liu","userId":"11072000168371721717"},"user_tz":240},"id":"oOuFlB38oxs3","outputId":"4d0a7d6f-10e3-4e99-8671-746a9664cfaf"},"outputs":[],"source":["print(data_tensor.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZqEh3GU9NjkG"},"outputs":[],"source":["class GCN(torch.nn.Module):\n","    def __init__(self, in_channels, out_channels, num_edges, num_nodes):\n","        super(GCN, self).__init__()\n","\n","        self.num_nodes = num_nodes\n","\n","        # Initialize GCNConv layers\n","        self.conv1 = GCNConv(in_channels, 1)\n","\n","        self.conv_mu = GCNConv(1, out_channels)\n","        self.conv_logvar = GCNConv(1, out_channels)\n","\n","        # Decoder for adjacency matrix\n","        self.decoder_linear = torch.nn.Linear(num_nodes, 16)\n","        self.decoder_ReLU = torch.nn.ReLU()\n","        self.decoder_linear2 =  torch.nn.Linear(16, num_edges)\n","\n","    def reparameterize(self, mu, logvar):\n","        std = torch.exp(logvar / 2)\n","        eps = torch.randn_like(std)\n","        return mu + eps * std\n","\n","    def forward(self, x, edge_index):\n","        x = F.relu(self.conv1(x, edge_index))\n","        mu = self.conv_mu(x, edge_index)\n","        logvar = self.conv_logvar(x, edge_index)\n","        z = self.reparameterize(mu, logvar).T\n","\n","        # print(\"z shape after reparameterizing\", z.shape)\n","        decoder1 = self.decoder_linear(z)\n","        # print(\"z shape after linear\", z.shape)\n","        decoder2 = self.decoder_ReLU(decoder1)\n","        # print(\"z shape are relu\", z.shape)\n","        decoder_edge_index = self.decoder_linear2(decoder2)\n","        # print(\"decoder_edge_index shape\", decoder_edge_index.shape)\n","        return decoder_edge_index, mu, logvar, z\n","\n","    def encode(self, x, edge_index):\n","        x = F.relu(self.conv1(x, edge_index))\n","        mu = self.conv_mu(x, edge_index)\n","        logvar = self.conv_logvar(x, edge_index)\n","        z = self.reparameterize(mu, logvar).T\n","        return mu, logvar, z\n","\n","    def decode(self, z):\n","        decoder1 = self.decoder_linear(z)\n","        decoder2 = self.decoder_ReLU(decoder1)\n","        decoder_edge_index = self.decoder_linear2(decoder2)\n","        return decoder_edge_index\n","\n","\n","def loss_function(mse_loss_fn, recon_edge_index, true_edge_index, mu, logvar):\n","   # Initialize here\n","    BCE_adj = mse_loss_fn(recon_edge_index, true_edge_index)  # Correctly apply the loss function\n","    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    return BCE_adj + KLD\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":740,"status":"ok","timestamp":1714270971799,"user":{"displayName":"Sophie Liu","userId":"11072000168371721717"},"user_tz":240},"id":"IwnjyFgATdqy","outputId":"90680080-7661-4bdc-d911-cecb56eedbbc"},"outputs":[],"source":["#global data\n","num_nodes = len(get_kmer_dictionary(2))\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = GCN(1, 2, 29780, num_nodes)\n","model.load_state_dict(torch.load(\"/content/drive/MyDrive/10708/10708 Project/VAE.pt\"))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"executionInfo":{"elapsed":271,"status":"error","timestamp":1714270974682,"user":{"displayName":"Sophie Liu","userId":"11072000168371721717"},"user_tz":240},"id":"VC0ylWx52xih","outputId":"ae8e3b46-77ab-4cb8-b90a-0b723ae0b8ab"},"outputs":[],"source":["train_embeddings = []\n","for edge_index in train_data:\n","  x = torch.randn(num_nodes, 1)\n","  _,_,z = model.encode(x, edge_index)\n","  train_embeddings.append(z.flatten())\n","  print(z.flatten().shape)\n","\n","data_tensor = torch.stack(train_embeddings)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1714268045204,"user":{"displayName":"Sophie Liu","userId":"11072000168371721717"},"user_tz":240},"id":"V3-0ZWuw4fjh","outputId":"ce9389f6-ad4b-495b-f92f-e77afa32727a"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","import torch\n","import torch.nn as nn\n","\n","class Discriminator(nn.Module):\n","    def __init__(self, embedding_dim):\n","        super(Discriminator, self).__init__()\n","        # Define each layer separately\n","\n","        self.linear1 = nn.Linear(embedding_dim, 256)\n","        self.leaky_relu1 = nn.LeakyReLU(0.2)\n","        self.linear2 = nn.Linear(256, 128)\n","        self.leaky_relu2 = nn.LeakyReLU(0.2)\n","        self.linear3 = nn.Linear(128, 1)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","\n","        # Manually connect each layer\n","        # print(\"discriminator a\", x.shape)\n","        x = self.leaky_relu1(self.linear1(x))\n","        # print(\"discriminator b\", x.shape)\n","        x = self.leaky_relu2(self.linear2(x))\n","        x = self.sigmoid(self.linear3(x))\n","\n","        return x\n","\n","# Example usage:\n","embedding_dim = 50  # Example embedding dimension\n","discriminator = Discriminator(embedding_dim)\n","\n","# Example input tensor (batch size of 1)\n","x = torch.randn(1, embedding_dim)\n","\n","# Generate output\n","output = discriminator(x)\n","print(\"Output:\", output)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1714268593516,"user":{"displayName":"Sophie Liu","userId":"11072000168371721717"},"user_tz":240},"id":"ZGwuFm78KfKm","outputId":"e942ea32-3611-4021-f9ed-ba06292fe411"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","class Generator(nn.Module):\n","    def __init__(self, input_dim, embedding_dim):\n","        super(Generator, self).__init__()\n","        self.embedding_dim = embedding_dim\n","\n","        # Define each layer separately\n","        self.linear1 = nn.Linear(input_dim, 128)\n","        self.leaky_relu1 = nn.LeakyReLU(0.2)\n","        self.linear2 = nn.Linear(128, 256)\n","        self.leaky_relu2 = nn.LeakyReLU(0.2)\n","        self.linear3 = nn.Linear(256, embedding_dim)\n","        self.tanh = nn.Softplus()\n","\n","    def forward(self, z):\n","        # Manually connect each layer\n","        # print(\"a\", z.shape)\n","        z = self.leaky_relu1(self.linear1(z))\n","        # print(\"b\", z.shape)\n","        z = self.leaky_relu2(self.linear2(z))\n","        z = self.tanh(self.linear3(z))\n","\n","        # Reshape the output to match the desired dimensions\n","        return z\n","\n","# Example usage:\n","input_dim = 100  # Size of the input latent vector\n","embedding_dim = 50  # Ensure this is divisible by 2 as it needs to be reshaped into two columns\n","generator = Generator(input_dim, embedding_dim)\n","\n","# Generate a random noise vector\n","z = torch.randn(1, input_dim)\n","\n","# Generate output\n","output = generator(z)\n","print(\"Output shape:\", output.shape)\n","\n","# Example usage:\n","latent_dim = 100\n","embedding_dim = 50  # This is the 25 in your data shape [25, 2]\n","generator = Generator(input_dim=latent_dim, embedding_dim=50)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kIeHbT9cK9a1"},"outputs":[],"source":["criterion = nn.BCELoss()\n","d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","g_optimizer = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M9KhAuuvBpxI"},"outputs":[],"source":["num_samples = data_tensor.size(0)\n","split_idx = int(num_samples * 0.8)\n","\n","# Split the tensor\n","train_tensor = data_tensor[:split_idx]\n","test_tensor = data_tensor[split_idx:]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1714266490703,"user":{"displayName":"Sophie Liu","userId":"11072000168371721717"},"user_tz":240},"id":"QIhCt-d7Pk1p","outputId":"109fa3ad-273c-42c4-a655-ba7fb2adea16"},"outputs":[],"source":["print(train_tensor.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":208033,"status":"ok","timestamp":1714268825919,"user":{"displayName":"Sophie Liu","userId":"11072000168371721717"},"user_tz":240},"id":"ecycxmbVLA-M","outputId":"25a41733-3e82-4123-a3df-03b8b7e2989e"},"outputs":[],"source":["num_epochs = 5000\n","for epoch in range(num_epochs):\n","    for real_data in train_tensor:  # Assuming real_data is loaded from somewhere\n","        batch_size = 1\n","        real_labels = torch.ones(batch_size, 1)\n","        fake_labels = torch.zeros(batch_size, 1)  # Correct the size to match real_labels\n","\n","        # Train Discriminator with real data\n","        d_optimizer.zero_grad()\n","        real_data = real_data.unsqueeze(0) if len(real_data.shape) == 1 else real_data  # Ensure real_data is at least 2D\n","        outputs = discriminator(real_data)\n","        d_loss_real = criterion(outputs, real_labels)\n","        # d_loss_real.backward(retain_graph=True)  # retain_graph as further backward calls are expected\n","\n","        # Train Discriminator with fake data\n","        noise = torch.randn(batch_size, latent_dim)\n","        fake_data = generator(noise)\n","        outputs = discriminator(fake_data.detach())  # detach to avoid training generator now\n","        d_loss_fake = criterion(outputs, fake_labels)\n","        d_loss_fake.backward(retain_graph=True)\n","        d_optimizer.step()\n","\n","        # Train Generator\n","        g_optimizer.zero_grad()\n","        outputs = discriminator(fake_data)  # no detach here, we want gradients to flow here\n","        g_loss = criterion(outputs, real_labels)  # Trick the discriminator\n","        g_loss.backward()  # no need to retain_graph here as it's the last backward call in the loop\n","        g_optimizer.step()\n","\n","    print(f\"Epoch {epoch+1}/{num_epochs}, D Loss: {d_loss_real.item() + d_loss_fake.item()}, G Loss: {g_loss.item()}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OjUhU9HMXjJ6"},"outputs":[],"source":["model = GCN(1, 2, 29780, num_nodes)\n","model.load_state_dict(torch.load(\"/content/drive/MyDrive/10708/10708 Project/VAE.pt\"))\n","\n","generated_sequences = []\n","kmer_dict = get_kmer_dictionary(2)\n","int_to_kmer = {value: key for key, value in kmer_dict.items()}\n","\n","\n","for i in range(1):\n","  noise = torch.randn(1, 100)\n","  fake_data = generator(noise)\n","  outputs = discriminator(fake_data.detach())\n","\n","  generated_edge_list = (model.decode(fake_data.reshape(2,25)))\n","\n","  generated_seq = \"\"\n","  for i in range(25): #this should be a straight path, i.e. i can just read the first row\n","    node = int(generated_edge_list[0][i]) #going to have to round\n","    kmer = int_to_kmer[node]\n","    generated_seq += (kmer)\n","  generated_sequences.append(generated_seq)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1714274000656,"user":{"displayName":"Sophie Liu","userId":"11072000168371721717"},"user_tz":240},"id":"-7DeMp4EpJAI","outputId":"c19cc122-3177-4f8c-f1d1-de98be850352"},"outputs":[],"source":["generated_sequences"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1FNXRDG1phwC"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b8YKamn8rf9f"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMg3WtAYZoSV99S2e9GNebG","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
